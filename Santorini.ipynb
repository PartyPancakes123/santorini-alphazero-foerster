{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d090f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.4\n",
      "2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101d35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Santorini:\n",
    "    def __init__(self):\n",
    "        self.row_count = 5\n",
    "        self.column_count = 5\n",
    "        self.action_size = 128\n",
    "        \n",
    "        self.adjacent = [(-1,-1), (-1, 0), (-1, 1), (0, -1), (0,  1), (1, -1), (1,  0), (1,  1)]\n",
    "\n",
    "        ################### BASIC BOARD SETUP - did not yet implement placing the pawns ###################\n",
    "        self.start_board = np.array([[0 for _ in range(5)] for _ in range(5)])\n",
    "        self.start_board[1, 1] = 110\n",
    "        self.start_board[3, 3] = 120\n",
    "        self.start_board[1, 3] = -110\n",
    "        self.start_board[3, 1] = -120\n",
    "        ###################################################################################################\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Santorini\"\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return self.start_board.copy()\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        index, move, build = self.readable_action_format(action)\n",
    "        piece_position = self.find_pawn_pos(state, player, index)\n",
    "        move_position = (piece_position[0] + self.adjacent[move][0], piece_position[1] + self.adjacent[move][1])\n",
    "        build_position = (move_position[0] + self.adjacent[build][0], move_position[1] + self.adjacent[build][1])\n",
    "\n",
    "        state[move_position] = player * (abs(state[piece_position]) // 10 * 10 + state[move_position])\n",
    "        state[piece_position] = abs(state[piece_position]) % 10\n",
    "        state[build_position] = state[build_position] + 1\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        pieces = self.find_player_pawns(state, 1)\n",
    "        actions = np.zeros(128)\n",
    "        for piece_position in pieces:\n",
    "            piece_index = state[piece_position] // 10 % 10 - 1\n",
    "            for move in range(8):\n",
    "                move_position = (piece_position[0] + self.adjacent[move][0], piece_position[1] + self.adjacent[move][1])\n",
    "\n",
    "                # check if valid move\n",
    "                if ((move_position[0] >= 0 and move_position[0] <= 4 and move_position[1] >= 0 and move_position[1] <= 4)   # valid cell\n",
    "                        and (state[move_position] >= 0 and state[move_position] < 4)                                        # not occupied / dome\n",
    "                        and state[move_position] - abs(state[piece_position]) % 10 <= 1):                                   # not too tall\n",
    "                    \n",
    "                    for build in range(8):\n",
    "                        build_position = (move_position[0] + self.adjacent[build][0], move_position[1] + self.adjacent[build][1])\n",
    "                        if build_position[0] >= 0 and build_position[0] <= 4 and build_position[1] >= 0 and build_position[1] <= 4:\n",
    "                            if (state[build_position] >= 0 and state[build_position] < 4) or build_position == piece_position:\n",
    "                                actions[self.machine_action_format(piece_index, move, build)] = 1\n",
    "        return actions\n",
    "    \n",
    "    def check_win(self, state, action, enemy_player=1):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        winners = [element for _, element in np.ndenumerate(state) if abs(element) > 100 and abs(element) % 10 == 3]\n",
    "        if len(winners) > 0 or self.no_possible_moves(state, player=enemy_player):\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def get_value_and_terminated(self, state, action, player=-1):\n",
    "        if self.check_win(state, action, enemy_player=self.get_opponent(player)):\n",
    "            return 1, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return np.where(abs(state) < 100, state, state*player)\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_neg_player = np.where(state < -100, abs(state) // 10 % 10, 0)\n",
    "        encoded_builds = np.where(abs(state) <= 4, state - (state//4)*5, 0)\n",
    "        encoded_pos_player = np.where(state > 100, state // 10 % 10, 0)\n",
    "\n",
    "        encoded_state = np.stack(\n",
    "            (encoded_neg_player, encoded_builds, encoded_pos_player)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state\n",
    "        \n",
    "    ############################################################ own functions:\n",
    "    \n",
    "    def no_possible_moves(self, state, player):\n",
    "        pieces = self.find_player_pawns(state, player)\n",
    "        for piece_position in pieces:\n",
    "            for move in range(8):\n",
    "                move_position = (piece_position[0] + self.adjacent[move][0], piece_position[1] + self.adjacent[move][1])\n",
    "\n",
    "                # check if valid move\n",
    "                if ((move_position[0] >= 0 and move_position[0] <= 4 and move_position[1] >= 0 and move_position[1] <= 4)   # valid cell\n",
    "                        and state[move_position] >= 0 and state[move_position] < 4                                          # not occupied / dome\n",
    "                        and state[move_position] - abs(state[piece_position]) % 10 <= 1):                                   # not too tall\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def readable_action_format(self, number):\n",
    "        index = number // 64\n",
    "        number = number % 64\n",
    "        move = number // 8\n",
    "        number = number % 8\n",
    "        build = number\n",
    "        return (index, move, build)\n",
    "    \n",
    "    def machine_action_format(self, piece_index, move, build):\n",
    "        return piece_index * 64 + move * 8 + build\n",
    "    \n",
    "    def find_pawn_pos(self, state, player, index):\n",
    "        position = [idx for idx, element in np.ndenumerate(state) if abs(element) > 100 and player * element > 0 and abs(element) // 10 % 10 == (index+1) ]\n",
    "        return position[0]\n",
    "        \n",
    "    def find_player_pawns(self, state, player):\n",
    "        return [idx for idx, element in np.ndenumerate(state) if player*element > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a097e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682c4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action, player=0):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e5b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21866526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        child = None\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return None\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b28ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action, player=player)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"test_model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"test_optimizer_{iteration}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "844e372e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316f675b2d52461f91316eaca0b7802c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 23\u001b[0m\n\u001b[0;32m      9\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_searches\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m500\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdirichlet_alpha\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.3\u001b[39m\n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     22\u001b[0m alphaZero \u001b[39m=\u001b[39m AlphaZero(model, optimizer, game, args)\n\u001b[1;32m---> 23\u001b[0m alphaZero\u001b[39m.\u001b[39;49mlearn()\n",
      "Cell \u001b[1;32mIn[28], line 69\u001b[0m, in \u001b[0;36mAlphaZero.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m selfPlay_iteration \u001b[39min\u001b[39;00m trange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mnum_selfPlay_iterations\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m---> 69\u001b[0m     memory \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselfPlay()\n\u001b[0;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m trange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mnum_epochs\u001b[39m\u001b[39m'\u001b[39m]):\n",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m, in \u001b[0;36mAlphaZero.selfPlay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     neutral_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mchange_perspective(state, player)\n\u001b[1;32m---> 16\u001b[0m     action_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmcts\u001b[39m.\u001b[39;49msearch(neutral_state)\n\u001b[0;32m     18\u001b[0m     memory\u001b[39m.\u001b[39mappend((neutral_state, action_probs, player))\n\u001b[0;32m     20\u001b[0m     temperature_action_probs \u001b[39m=\u001b[39m action_probs \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[27], line 91\u001b[0m, in \u001b[0;36mMCTS.search\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     88\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_opponent_value(value)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_terminal:\n\u001b[1;32m---> 91\u001b[0m     policy, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m     92\u001b[0m         torch\u001b[39m.\u001b[39;49mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mget_encoded_state(node\u001b[39m.\u001b[39;49mstate), device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdevice)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     94\u001b[0m     policy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(policy, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     95\u001b[0m     valid_moves \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_valid_moves(node\u001b[39m.\u001b[39mstate)\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[26], line 36\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstartBlock(x)\n\u001b[0;32m     37\u001b[0m     \u001b[39mfor\u001b[39;00m resBlock \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackBone:\n\u001b[0;32m     38\u001b[0m         x \u001b[39m=\u001b[39m resBlock(x)\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game = Santorini()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 256, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 500,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 200,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76798ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0]\n",
      " [   0  110    1    1    0]\n",
      " [   0    0 -110  122    0]\n",
      " [   0    1    0    3    0]\n",
      " [   0 -120    1    0    0]]\n",
      "expected (52) : 0.0017213871469721198 - (0, 6, 4)\n",
      "instead the max is 100 : 0.0552792064845562 - (1, 4, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkkUlEQVR4nO3df1CVZf7/8Rc/hKMWaDByxPCDFRv+IExJwppxdzojNMwa2y4q6yaxjk2tbBQ75I9VaIZt0XZ0tWBkbXKr2UzXGXNLzR06/qhWkgTccktzZzNc7YDWCoYJLuf+/rHfTp48GAeVc3F4Pmbu8Zz7ft8313WB8vK67/vcIZZlWQIAADBYaKAbAAAA8F0ILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA44UHugFXg9vt1smTJ3X99dcrJCQk0M0BAAA9YFmWzp49q/j4eIWGXn4OJSgCy8mTJ5WQkBDoZgAAgF44fvy4brzxxsvWBEVguf766yX9r8NRUVEBbg0AAOiJtrY2JSQkeH6PX05QBJavTwNFRUURWAAA6Gd6cjkHF90CAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGC880A0AAOByEhdt97w+tjw7gC1BIDHDAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABgvPNANAADgYomLtnteH1ueHcCWwCTMsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgvF4FlqqqKiUmJspmsyk9PV11dXWXrd+8ebOSk5Nls9mUkpKiHTt2eG1/8MEHFRIS4rVkZWX1pmkAACAI+R1YNm3apOLiYpWVlamhoUGpqanKzMxUS0uLz/p9+/YpLy9P8+bNU2Njo3JycpSTk6NDhw551WVlZemzzz7zLK+88krvegQAAIKO34Fl1apVmj9/vgoKCjRu3DhVV1dryJAhWr9+vc/6NWvWKCsrSyUlJRo7dqzKy8s1adIkVVZWetVFRkbKbrd7luHDh/euRwAAIOj4FVg6OztVX18vh8PxzQFCQ+VwOFRbW+tzn9raWq96ScrMzLykfs+ePRoxYoRuvfVWPfLII/r888+7bUdHR4fa2tq8FgAAELz8CiynT59WV1eX4uLivNbHxcXJ5XL53Mflcn1nfVZWll566SU5nU6tWLFCe/fu1b333quuri6fx6yoqFB0dLRnSUhI8KcbAACgnwkPdAMkafbs2Z7XKSkpuu2223TzzTdrz549uueeey6pX7x4sYqLiz3v29raCC0AAAQxv2ZYYmNjFRYWpubmZq/1zc3NstvtPvex2+1+1UvSTTfdpNjYWP3zn//0uT0yMlJRUVFeCwAACF5+BZaIiAhNnjxZTqfTs87tdsvpdCojI8PnPhkZGV71klRTU9NtvST9+9//1ueff66RI0f60zwAABCk/L5LqLi4WM8995xefPFFffTRR3rkkUfU3t6ugoICSdLcuXO1ePFiT31RUZF27typlStX6vDhw3ryySd14MABFRYWSpK+/PJLlZSU6N1339WxY8fkdDp133336ZZbblFmZuZV6iYAAOjP/L6GZdasWTp16pRKS0vlcrk0ceJE7dy503NhbVNTk0JDv8lBU6dO1YYNG7R06VItWbJESUlJ2rp1qyZMmCBJCgsL0/vvv68XX3xRZ86cUXx8vKZPn67y8nJFRkZepW4CAID+LMSyLCvQjbhSbW1tio6OVmtrK9ezAEA/l7hou+f1seXZl7xH8PDn9zfPEgIAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGM+Ihx8CAAYWPlsF/mKGBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOP1KrBUVVUpMTFRNptN6enpqquru2z95s2blZycLJvNppSUFO3YsaPb2ocfflghISFavXp1b5oGAACCkN+BZdOmTSouLlZZWZkaGhqUmpqqzMxMtbS0+Kzft2+f8vLyNG/ePDU2NionJ0c5OTk6dOjQJbWvvvqq3n33XcXHx/vfEwAAELT8DiyrVq3S/PnzVVBQoHHjxqm6ulpDhgzR+vXrfdavWbNGWVlZKikp0dixY1VeXq5JkyapsrLSq+7EiRP65S9/qZdfflmDBg3qXW8AAEBQ8iuwdHZ2qr6+Xg6H45sDhIbK4XCotrbW5z61tbVe9ZKUmZnpVe92u/XAAw+opKRE48eP96dJAABgAAj3p/j06dPq6upSXFyc1/q4uDgdPnzY5z4ul8tnvcvl8rxfsWKFwsPD9eijj/aoHR0dHero6PC8b2tr62kXAABAPxTwu4Tq6+u1Zs0avfDCCwoJCenRPhUVFYqOjvYsCQkJ17iVAAAgkPwKLLGxsQoLC1Nzc7PX+ubmZtntdp/72O32y9a//fbbamlp0ejRoxUeHq7w8HB9+umn+tWvfqXExESfx1y8eLFaW1s9y/Hjx/3pBgAA6Gf8CiwRERGaPHmynE6nZ53b7ZbT6VRGRobPfTIyMrzqJammpsZT/8ADD+j999/XwYMHPUt8fLxKSkr017/+1ecxIyMjFRUV5bUAAIDg5dc1LJJUXFys/Px8paWlacqUKVq9erXa29tVUFAgSZo7d65GjRqliooKSVJRUZGmTZumlStXKjs7Wxs3btSBAwe0bt06SVJMTIxiYmK8vsagQYNkt9t16623Xmn/AABAEPA7sMyaNUunTp1SaWmpXC6XJk6cqJ07d3ourG1qalJo6DcTN1OnTtWGDRu0dOlSLVmyRElJSdq6dasmTJhw9XoBAACCmt+BRZIKCwtVWFjoc9uePXsuWZebm6vc3NweH//YsWO9aRYAAAhSAb9LCAAA4LsQWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMF54oBsAAANV4qLtntfHlmcHsCWA+ZhhAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMxwfHAcAAwQfVoT9jhgUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeL0KLFVVVUpMTJTNZlN6errq6uouW79582YlJyfLZrMpJSVFO3bs8Nr+5JNPKjk5WUOHDtXw4cPlcDi0f//+3jQNAAAEIb8Dy6ZNm1RcXKyysjI1NDQoNTVVmZmZamlp8Vm/b98+5eXlad68eWpsbFROTo5ycnJ06NAhT833vvc9VVZW6oMPPtA777yjxMRETZ8+XadOnep9zwAAQNDwO7CsWrVK8+fPV0FBgcaNG6fq6moNGTJE69ev91m/Zs0aZWVlqaSkRGPHjlV5ebkmTZqkyspKT81Pf/pTORwO3XTTTRo/frxWrVqltrY2vf/++73vGQAACBp+BZbOzk7V19fL4XB8c4DQUDkcDtXW1vrcp7a21qtekjIzM7ut7+zs1Lp16xQdHa3U1FSfNR0dHWpra/NaAABA8PIrsJw+fVpdXV2Ki4vzWh8XFyeXy+VzH5fL1aP6bdu26brrrpPNZtPvf/971dTUKDY21ucxKyoqFB0d7VkSEhL86QYAAOhnjLlL6Ac/+IEOHjyoffv2KSsrSzNnzuz2upjFixertbXVsxw/fryPWwsAAPqSX4ElNjZWYWFham5u9lrf3Nwsu93ucx+73d6j+qFDh+qWW27RnXfeqeeff17h4eF6/vnnfR4zMjJSUVFRXgsAAAhefgWWiIgITZ48WU6n07PO7XbL6XQqIyPD5z4ZGRle9ZJUU1PTbf3Fx+3o6PCneQAAIEiF+7tDcXGx8vPzlZaWpilTpmj16tVqb29XQUGBJGnu3LkaNWqUKioqJElFRUWaNm2aVq5cqezsbG3cuFEHDhzQunXrJEnt7e166qmnNGPGDI0cOVKnT59WVVWVTpw4odzc3KvYVQAA0F/5HVhmzZqlU6dOqbS0VC6XSxMnTtTOnTs9F9Y2NTUpNPSbiZupU6dqw4YNWrp0qZYsWaKkpCRt3bpVEyZMkCSFhYXp8OHDevHFF3X69GnFxMTojjvu0Ntvv63x48dfpW4CAID+zO/AIkmFhYUqLCz0uW3Pnj2XrMvNze12tsRms2nLli29aQYAABggehVYAADmS1y03fP62PLsALYEuHLG3NYMAADQHQILAAAwHoEFAAAYj8ACAACMx0W3ABBELr7QFggmzLAAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAOiBxEXblbhoe6CbAQxYBBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILAAAwHgEFgAAYDwCCwAAMB6BBQAAGI/AAgAAjEdgAQAAxiOwAAAA4xFYAACA8QgsAAC/JC7arsRF2wPdDAwwBBYAAGA8AgsAADAegQUAABiPwAIAAIxHYAEAAMYjsAAAAOOFB7oBgD8uvpXy2PLsALYEuLr42QYujxkWAABgPAILAAAwHoEFAAAYj8ACAACM16vAUlVVpcTERNlsNqWnp6uuru6y9Zs3b1ZycrJsNptSUlK0Y8cOz7YLFy5o4cKFSklJ0dChQxUfH6+5c+fq5MmTvWkaAAAIQn4Hlk2bNqm4uFhlZWVqaGhQamqqMjMz1dLS4rN+3759ysvL07x589TY2KicnBzl5OTo0KFDkqRz586poaFBy5YtU0NDg7Zs2aIjR45oxowZV9YzYAD7+uF0PKAOQLDwO7CsWrVK8+fPV0FBgcaNG6fq6moNGTJE69ev91m/Zs0aZWVlqaSkRGPHjlV5ebkmTZqkyspKSVJ0dLRqamo0c+ZM3XrrrbrzzjtVWVmp+vp6NTU1XVnvAABAUPArsHR2dqq+vl4Oh+ObA4SGyuFwqLa21uc+tbW1XvWSlJmZ2W29JLW2tiokJETDhg3zub2jo0NtbW1eCwAACF5+BZbTp0+rq6tLcXFxXuvj4uLkcrl87uNyufyqP3/+vBYuXKi8vDxFRUX5rKmoqFB0dLRnSUhI8KcbAACgnzHqLqELFy5o5syZsixLa9eu7bZu8eLFam1t9SzHjx/vw1YCAIC+5tdH88fGxiosLEzNzc1e65ubm2W3233uY7fbe1T/dVj59NNPtWvXrm5nVyQpMjJSkZGR/jQdAAD0Y37NsERERGjy5MlyOp2edW63W06nUxkZGT73ycjI8KqXpJqaGq/6r8PK0aNH9eabbyomJsafZgEAgCDn98MPi4uLlZ+fr7S0NE2ZMkWrV69We3u7CgoKJElz587VqFGjVFFRIUkqKirStGnTtHLlSmVnZ2vjxo06cOCA1q1bJ+l/YeUnP/mJGhoatG3bNnV1dXmub7nhhhsUERFxtfoKAAD6Kb8Dy6xZs3Tq1CmVlpbK5XJp4sSJ2rlzp+fC2qamJoWGfjNxM3XqVG3YsEFLly7VkiVLlJSUpK1bt2rChAmSpBMnTui1116TJE2cONHra+3evVvf//73e9k1AAAQLPwOLJJUWFiowsJCn9v27Nlzybrc3Fzl5ub6rE9MTJRlWb1pBgAAGCCMuksIAADAFwILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADBerz6aHwAQXBIXbfe8PrY8O4AtAXxjhgUAABiPwAIAAIxHYAEAAMYjsAAAAOMRWAAAgPEILACASyQu2u515xAQaAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB44YFuAACg9y7+cLdjy7MD2BLg2mKGBQAAGI/AAgAAjMcpIQD9DqdBgIGHGRYAAGA8AgsA/H88oRgwF4EFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4BBYAAGA8AgsAADAegQUAABiPZwkBQB/hGUhA7zHDAgAAjMcMCwAg4HiGE74LgQVXDdPdAIBrhcACvxBKAACBwDUsAADAeAQWAABgPAILAAAwHtewoE9w7QsA4EowwwIAAIxHYAEAAMbjlBCAfo9TjkDw69UMS1VVlRITE2Wz2ZSenq66urrL1m/evFnJycmy2WxKSUnRjh07vLZv2bJF06dPV0xMjEJCQnTw4MHeNAsAAAQpvwPLpk2bVFxcrLKyMjU0NCg1NVWZmZlqaWnxWb9v3z7l5eVp3rx5amxsVE5OjnJycnTo0CFPTXt7u+6++26tWLGi9z0BAABBy+/AsmrVKs2fP18FBQUaN26cqqurNWTIEK1fv95n/Zo1a5SVlaWSkhKNHTtW5eXlmjRpkiorKz01DzzwgEpLS+VwOHrfEwAAELT8CiydnZ2qr6/3ChahoaFyOByqra31uU9tbe0lQSQzM7PbegDozxIXbfcsAK4evy66PX36tLq6uhQXF+e1Pi4uTocPH/a5j8vl8lnvcrn8bOo3Ojo61NHR4Xnf1tbW62MBAADz9cvbmisqKhQdHe1ZEhISAt0kAABwDfkVWGJjYxUWFqbm5mav9c3NzbLb7T73sdvtftX3xOLFi9Xa2upZjh8/3utjAQAA8/kVWCIiIjR58mQ5nU7POrfbLafTqYyMDJ/7ZGRkeNVLUk1NTbf1PREZGamoqCivBQAABC+/PziuuLhY+fn5SktL05QpU7R69Wq1t7eroKBAkjR37lyNGjVKFRUVkqSioiJNmzZNK1euVHZ2tjZu3KgDBw5o3bp1nmN+8cUXampq0smTJyVJR44ckfS/2ZkrmYkBAADBwe/AMmvWLJ06dUqlpaVyuVyaOHGidu7c6bmwtqmpSaGh30zcTJ06VRs2bNDSpUu1ZMkSJSUlaevWrZowYYKn5rXXXvMEHkmaPXu2JKmsrExPPvlkb/uGi/BJoACA/qxXH81fWFiowsJCn9v27Nlzybrc3Fzl5uZ2e7wHH3xQDz74YG+aAgAABoB+eZcQzMHnTQAA+gKBBQAAGI/AAgAAjEdgAdAtTvkBMAWBBQAAGK9XdwkBpuM2bgAILsywAAAA4xFYAACA8TglBAA+cFoRMAszLAAAwHgEFgAAYDwCCwAAMB7XsAAYsLhOBeg/mGEBAADGY4YFxuF/vQCAbyOwAEA/w/OdMBARWIA+xgwSAPiPwBLE+F8YACBYcNEtAAAwHjMsuASnLAAApiGwAAD6BKepcSU4JQQAAIxHYAEAAMbjlJCBuIYECB6cBgGuDgILjEZ4w5XiZwgIDpwSAgAAxmOGBTAIswH909ffN75nwLXDDAvQBxIXbedaBgC4AgQWAABgPAILAAAwHtew9ANc1wAAGOgILEGEYAMACFacEgIAAMZjhqWfYRale9yFAwDBi8ACBFhvgxbhFcBAQmABv/gAAMbjGhYAAGA8ZliAa4BZKwC4uphhAQAAxmOGBRhgmP0B0B8RWBAQ/NIEAPiDU0IAAMB4BBZ8p8RF2/lQNgBAQBFYAACA8QgsAADAeAQWAABgPO4SAgYw7tYC0F8QWACDESgA4H8ILEAQINgACHYEFvRb/JIGgIGDi24BAIDxmGEB0CPMaAEIJGZYAACA8XoVWKqqqpSYmCibzab09HTV1dVdtn7z5s1KTk6WzWZTSkqKduzY4bXdsiyVlpZq5MiRGjx4sBwOh44ePdqbpgEAgCDkd2DZtGmTiouLVVZWpoaGBqWmpiozM1MtLS0+6/ft26e8vDzNmzdPjY2NysnJUU5Ojg4dOuSpefrpp/XMM8+ourpa+/fv19ChQ5WZmanz58/3vmcAACBo+B1YVq1apfnz56ugoEDjxo1TdXW1hgwZovXr1/usX7NmjbKyslRSUqKxY8eqvLxckyZNUmVlpaT/za6sXr1aS5cu1X333afbbrtNL730kk6ePKmtW7deUecAAEBw8Oui287OTtXX12vx4sWedaGhoXI4HKqtrfW5T21trYqLi73WZWZmesLIJ598IpfLJYfD4dkeHR2t9PR01dbWavbs2Zccs6OjQx0dHZ73ra2tkqS2tjZ/umMsd8c5z+u2tjav9xf7dn+vxX4Xv/d3mz9fo7vab7e7t9v6Wm/G8Nuu1jj5s193/fiuY/a13v489fTn8rt+1rtrz5V8f7+97XKuxt8Rf8aiu6/v78/BtWg3+revv5+WZX13seWHEydOWJKsffv2ea0vKSmxpkyZ4nOfQYMGWRs2bPBaV1VVZY0YMcKyLMv629/+ZkmyTp486VWTm5trzZw50+cxy8rKLEksLCwsLCwsQbAcP378OzNIv7ytefHixV6zNm63W1988YViYmIUEhJy1b9eW1ubEhISdPz4cUVFRV314/dnjE33GBvfGJfuMTa+MS7d6+9jY1mWzp49q/j4+O+s9SuwxMbGKiwsTM3NzV7rm5ubZbfbfe5jt9svW//1n83NzRo5cqRXzcSJE30eMzIyUpGRkV7rhg0b5k9XeiUqKqpf/kD0Bcame4yNb4xL9xgb3xiX7vXnsYmOju5RnV8X3UZERGjy5MlyOp2edW63W06nUxkZGT73ycjI8KqXpJqaGk/9mDFjZLfbvWra2tq0f//+bo8JAAAGFr9PCRUXFys/P19paWmaMmWKVq9erfb2dhUUFEiS5s6dq1GjRqmiokKSVFRUpGnTpmnlypXKzs7Wxo0bdeDAAa1bt06SFBISoscee0y/+c1vlJSUpDFjxmjZsmWKj49XTk7O1espAADot/wOLLNmzdKpU6dUWloql8uliRMnaufOnYqLi5MkNTU1KTT0m4mbqVOnasOGDVq6dKmWLFmipKQkbd26VRMmTPDUPPHEE2pvb9dDDz2kM2fO6O6779bOnTtls9muQhevXGRkpMrKyi45DQXG5nIYG98Yl+4xNr4xLt0bSGMTYlk9uZcIAAAgcHiWEAAAMB6BBQAAGI/AAgAAjEdgAQAAxiOw9EBVVZUSExNls9mUnp6uurq6QDepT1VUVOiOO+7Q9ddfrxEjRignJ0dHjhzxqjl//rwWLFigmJgYXXfddfrxj398yQcGBrvly5d7btP/2kAelxMnTuhnP/uZYmJiNHjwYKWkpOjAgQOe7ZZlqbS0VCNHjtTgwYPlcDh09OjRALa4b3R1dWnZsmUaM2aMBg8erJtvvlnl5eVez1IZKGPz1ltv6Yc//KHi4+MVEhJyyQNvezIOX3zxhebMmaOoqCgNGzZM8+bN05dfftmHvbj6LjcuFy5c0MKFC5WSkqKhQ4cqPj5ec+fO1cmTJ72OEYzjQmD5Dps2bVJxcbHKysrU0NCg1NRUZWZmqqWlJdBN6zN79+7VggUL9O6776qmpkYXLlzQ9OnT1d7e7ql5/PHH9frrr2vz5s3au3evTp48qfvvvz+Are5b7733nv7whz/otttu81o/UMflP//5j+666y4NGjRIb7zxhj788EOtXLlSw4cP99Q8/fTTeuaZZ1RdXa39+/dr6NChyszM1Pnz5wPY8mtvxYoVWrt2rSorK/XRRx9pxYoVevrpp/Xss896agbK2LS3tys1NVVVVVU+t/dkHObMmaN//OMfqqmp0bZt2/TWW2/poYce6qsuXBOXG5dz586poaFBy5YtU0NDg7Zs2aIjR45oxowZXnXBOC5+PfxwIJoyZYq1YMECz/uuri4rPj7eqqioCGCrAqulpcWSZO3du9eyLMs6c+aMNWjQIGvz5s2emo8++siSZNXW1gaqmX3m7NmzVlJSklVTU2NNmzbNKioqsixrYI/LwoULrbvvvrvb7W6327Lb7dbvfvc7z7ozZ85YkZGR1iuvvNIXTQyY7Oxs6+c//7nXuvvvv9+aM2eOZVkDd2wkWa+++qrnfU/G4cMPP7QkWe+9956n5o033rBCQkKsEydO9Fnbr6Vvj4svdXV1liTr008/tSwreMeFGZbL6OzsVH19vRwOh2ddaGioHA6HamtrA9iywGptbZUk3XDDDZKk+vp6XbhwwWuckpOTNXr06AExTgsWLFB2drZX/6WBPS6vvfaa0tLSlJubqxEjRuj222/Xc88959n+ySefyOVyeY1NdHS00tPTg35spk6dKqfTqY8//liS9Pe//13vvPOO7r33XkkDe2wu1pNxqK2t1bBhw5SWluapcTgcCg0N1f79+/u8zYHS2tqqkJAQzzP1gnVc+uXTmvvK6dOn1dXV5fkU36/FxcXp8OHDAWpVYLndbj322GO66667PJ9W7HK5FBERcckDKOPi4uRyuQLQyr6zceNGNTQ06L333rtk20Ael3/9619au3atiouLtWTJEr333nt69NFHFRERofz8fE//ff3dCvaxWbRokdra2pScnKywsDB1dXXpqaee0pw5cyRpQI/NxXoyDi6XSyNGjPDaHh4erhtuuGHAjNX58+e1cOFC5eXleR5+GKzjQmCBXxYsWKBDhw7pnXfeCXRTAu748eMqKipSTU2NMY+RMIXb7VZaWpp++9vfSpJuv/12HTp0SNXV1crPzw9w6wLrz3/+s15++WVt2LBB48eP18GDB/XYY48pPj5+wI8N/HPhwgXNnDlTlmVp7dq1gW7ONccpocuIjY1VWFjYJXd1NDc3y263B6hVgVNYWKht27Zp9+7duvHGGz3r7Xa7Ojs7debMGa/6YB+n+vp6tbS0aNKkSQoPD1d4eLj27t2rZ555RuHh4YqLixuQ4yJJI0eO1Lhx47zWjR07Vk1NTZLk6f9A/LtVUlKiRYsWafbs2UpJSdEDDzygxx9/3PPA2IE8NhfryTjY7fZLboD473//qy+++CLox+rrsPLpp5+qpqbGM7siBe+4EFguIyIiQpMnT5bT6fSsc7vdcjqdysjICGDL+pZlWSosLNSrr76qXbt2acyYMV7bJ0+erEGDBnmN05EjR9TU1BTU43TPPffogw8+0MGDBz1LWlqa5syZ43k9EMdFku66665Lbn3/+OOP9X//93+SpDFjxshut3uNTVtbm/bv3x/0Y3Pu3DmvB8RKUlhYmNxut6SBPTYX68k4ZGRk6MyZM6qvr/fU7Nq1S263W+np6X3e5r7ydVg5evSo3nzzTcXExHhtD9pxCfRVv6bbuHGjFRkZab3wwgvWhx9+aD300EPWsGHDLJfLFeim9ZlHHnnEio6Otvbs2WN99tlnnuXcuXOemocfftgaPXq0tWvXLuvAgQNWRkaGlZGREcBWB8bFdwlZ1sAdl7q6Ois8PNx66qmnrKNHj1ovv/yyNWTIEOtPf/qTp2b58uXWsGHDrL/85S/W+++/b913333WmDFjrK+++iqALb/28vPzrVGjRlnbtm2zPvnkE2vLli1WbGys9cQTT3hqBsrYnD171mpsbLQaGxstSdaqVausxsZGz90uPRmHrKws6/bbb7f2799vvfPOO1ZSUpKVl5cXqC5dFZcbl87OTmvGjBnWjTfeaB08eNDr3+SOjg7PMYJxXAgsPfDss89ao0ePtiIiIqwpU6ZY7777bqCb1Kck+Vz++Mc/emq++uor6xe/+IU1fPhwa8iQIdaPfvQj67PPPgtcowPk24FlII/L66+/bk2YMMGKjIy0kpOTrXXr1nltd7vd1rJly6y4uDgrMjLSuueee6wjR44EqLV9p62tzSoqKrJGjx5t2Ww266abbrJ+/etfe/2yGShjs3v3bp//tuTn51uW1bNx+Pzzz628vDzruuuus6KioqyCggLr7NmzAejN1XO5cfnkk0+6/Td59+7dnmME47iEWNZFH68IAABgIK5hAQAAxiOwAAAA4xFYAACA8QgsAADAeAQWAABgPAILAAAwHoEFAAAYj8ACAACMR2ABAADGI7AAAADjEVgAAIDxCCwAAMB4/w9OteF7pnnfugAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "santorini = Santorini()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = santorini.get_initial_state()\n",
    "state = santorini.get_next_state(state, 71, 1)\n",
    "state = santorini.get_next_state(state, 28, -1)\n",
    "state = santorini.get_next_state(state, 121, 1)\n",
    "state = santorini.get_next_state(state, 49, -1)\n",
    "state = santorini.get_next_state(state, 78, 1)\n",
    "state = santorini.get_next_state(state, 120, -1)\n",
    "state = santorini.get_next_state(state, 113, 1)\n",
    "state = santorini.get_next_state(state, 92, -1)\n",
    "state = santorini.get_next_state(state, 78, 1)\n",
    "\n",
    "# this is the expected move to block the positive player from stepping down on to the level3 building\n",
    "expected = santorini.machine_action_format(0, 6, 4)\n",
    "\n",
    "\n",
    "neutral_state = santorini.change_perspective(state, -1)\n",
    "encoded_state = santorini.get_encoded_state(neutral_state)\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(santorini, 9, 256, device=device)\n",
    "# here going through the models 0 -> 3 we can see that the expected action's policy keeps going down\n",
    "model.load_state_dict(torch.load('model_1_Santorini.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "possible = santorini.get_valid_moves(neutral_state)\n",
    "policy *= possible\n",
    "\n",
    "# print(value)\n",
    "print(state)\n",
    "\n",
    "print(f\"expected ({expected}) : {policy[expected]} - {santorini.readable_action_format(expected)}\")\n",
    "max_a = np.argmax(policy)\n",
    "print(f\"instead the max is {max_a} : {policy[max_a]} - {santorini.readable_action_format(max_a)}\")\n",
    "\n",
    "\n",
    "plt.bar(range(santorini.action_size), policy)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
